#!/usr/bin/env python3
"""
Experiment 3: SVD Analysis - Geometric Explanation of Massive Activations

This experiment proves that massive activations arise because function words'
intermediate representations are highly aligned with the principal amplification
direction (top singular vector) of the down-projection matrix W‚ÇÇ.

Key Claims:
1. W‚ÇÇ has a dominant singular direction (œÉ‚ÇÅ >> œÉ‚ÇÇ)
2. Function words align more strongly with v‚ÇÅ than content words
3. Alignment strength predicts massive activation magnitude (causal relationship)

Mathematical Framework:
  W‚ÇÇ = U Œ£ V·µÄ  (SVD decomposition)
  v‚ÇÅ = top right singular vector (principal direction in 3072-dim space)
  For token t: alignment(t) = cos(Œ∏) between h‚ÇÇ[t] and v‚ÇÅ
  Prediction: massive_activation[t] ‚àù (h‚ÇÇ[t] ¬∑ v‚ÇÅ) √ó œÉ‚ÇÅ
"""

import os
import sys
import argparse
import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import seaborn as sns
from scipy import stats
from collections import defaultdict, Counter
import json
from datetime import datetime

# Add lib to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

import lib
import monkey_patch as mp


# ============================================================================
# FUNCTION WORD DEFINITIONS
# ============================================================================

FUNCTION_WORDS = {
    # Articles
    'the', 'a', 'an',

    # Prepositions
    'of', 'to', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
    'into', 'about', 'after', 'before', 'between', 'through', 'during',
    'under', 'over', 'against', 'within', 'without', 'among',

    # Conjunctions
    'and', 'or', 'but', 'if', 'that', 'which', 'while', 'because',
    'although', 'though', 'unless', 'until', 'since', 'when', 'where',

    # Pronouns
    'it', 'its', 'they', 'them', 'their', 'this', 'these', 'those',
    'he', 'she', 'his', 'her', 'him', 'we', 'us', 'our',

    # Auxiliary verbs
    'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did',
    'will', 'would', 'should', 'could', 'can', 'may', 'might', 'must',

    # Common adverbs
    'not', 'no', 'yes', 'also', 'just', 'only', 'very', 'so', 'too',
    'then', 'there', 'here', 'now',

    # Punctuation (as strings)
    '.', ',', '!', '?', ';', ':', '-', '(', ')', '[', ']', '"', "'",
}

# Add common variations
FUNCTION_WORDS_VARIATIONS = set()
for word in list(FUNCTION_WORDS):
    FUNCTION_WORDS_VARIATIONS.add(word)
    FUNCTION_WORDS_VARIATIONS.add(word.capitalize())
    FUNCTION_WORDS_VARIATIONS.add(word.upper())
    # Add with leading space (BPE tokens)
    FUNCTION_WORDS_VARIATIONS.add(' ' + word)
    FUNCTION_WORDS_VARIATIONS.add('ƒ†' + word)  # GPT-2 BPE marker

FUNCTION_WORDS = FUNCTION_WORDS_VARIATIONS


class SVDAlignmentTracker:
    """
    Track token activations and compute alignment with W‚ÇÇ's principal direction
    """
    def __init__(self, layer_id, tokenizer):
        self.layer_id = layer_id
        self.tokenizer = tokenizer

        # Store intermediate activations (after GELU)
        self.token_activations = []  # List of (token_text, h2, output)

        # Current processing
        self.current_tokens = None
        self.current_h2 = []
        self.current_output = []

    def track_gelu(self, module, input, output):
        """Capture activations after GELU"""
        h2 = output.detach().cpu().double()
        self.current_h2.append(h2)

    def track_output(self, module, input, output):
        """Capture MLP output"""
        mlp_output = output.detach().cpu().double()
        self.current_output.append(mlp_output)

    def finalize_sample(self, input_ids):
        """After processing one sample, decode tokens and store"""
        if not self.current_h2:
            return

        # Decode tokens
        tokens = [self.tokenizer.decode([tid]) for tid in input_ids[0].cpu().tolist()]

        # Concatenate all samples
        h2_tensor = torch.cat(self.current_h2, dim=0)  # [batch*seq, 3072]
        output_tensor = torch.cat(self.current_output, dim=0)  # [batch*seq, 768]

        # Flatten batch and sequence dimensions
        if len(h2_tensor.shape) == 3:  # [batch, seq, dim]
            h2_tensor = h2_tensor.view(-1, h2_tensor.shape[-1])
        if len(output_tensor.shape) == 3:
            output_tensor = output_tensor.view(-1, output_tensor.shape[-1])

        # Store token-level data
        num_tokens = min(len(tokens), h2_tensor.shape[0], output_tensor.shape[0])
        for i in range(num_tokens):
            self.token_activations.append({
                'token': tokens[i],
                'h2': h2_tensor[i],  # [3072]
                'output': output_tensor[i],  # [768]
            })

        # Clear for next sample
        self.current_h2 = []
        self.current_output = []


def perform_svd_analysis(model, args):
    """
    Phase 1: Perform SVD decomposition of W‚ÇÇ matrix
    """
    print(f"\n{'='*80}")
    print(f"PHASE 1: SVD DECOMPOSITION OF W‚ÇÇ (Layer {args.layer_id})")
    print(f"{'='*80}\n")

    # Extract W‚ÇÇ matrix (down-projection)
    # c_proj.weight in PyTorch is stored as [out_features, in_features]
    # For GPT-2: c_proj maps 3072 ‚Üí 768, so weight is [768, 3072]
    # BUT: The actual stored shape might be [3072, 768] depending on implementation

    layer = model.transformer.h[args.layer_id]
    W2_weight = layer.mlp.c_proj.weight.detach().cpu().double()

    print(f"W‚ÇÇ weight shape (as stored): {W2_weight.shape}")

    # Check actual shape and determine correct matrix
    if W2_weight.shape[0] == 3072 and W2_weight.shape[1] == 768:
        # Stored as [3072, 768], use directly
        # h2[3072] @ W2[3072, 768] ‚Üí output[768]
        W2 = W2_weight
        print(f"  Using as-is: h‚ÇÇ[3072] @ W‚ÇÇ[3072,768] ‚Üí output[768]")
    else:
        # Stored as [768, 3072], need transpose
        # h2[3072] @ W2.T[3072, 768] ‚Üí output[768]
        W2 = W2_weight.T
        print(f"  Transposed: h‚ÇÇ[3072] @ W‚ÇÇ.T[3072,768] ‚Üí output[768]")

    print(f"W‚ÇÇ effective shape for SVD: {W2.shape}")
    print(f"W‚ÇÇ dtype: {W2.dtype}")

    # Perform SVD: W‚ÇÇ = U Œ£ V·µÄ
    print("\nPerforming SVD decomposition...")
    U, S, Vh = torch.linalg.svd(W2, full_matrices=False)

    # For W2[3072, 768]:
    # U: [3072, 768] - left singular vectors (input space directions)
    # S: [768] - singular values (amplification factors)
    # Vh: [768, 768] - right singular vectors (output space directions)

    print(f"U shape: {U.shape}")
    print(f"S shape: {S.shape}")
    print(f"Vh shape: {Vh.shape}")

    # Extract top singular vector and value
    # U[:, 0] is the principal INPUT direction in 3072-dim intermediate space
    # This is the direction that gets amplified the most by W‚ÇÇ
    v1 = U[:, 0]  # Should be [3072]
    sigma1 = S[0].item()

    print(f"\nv‚ÇÅ shape: {v1.shape} (should be [3072])")
    assert v1.shape[0] == 3072, f"v1 should be 3072-dim, got {v1.shape}"

    print(f"\n{'‚îÄ'*60}")
    print(f"SINGULAR VALUE SPECTRUM")
    print(f"{'‚îÄ'*60}")
    print(f"œÉ‚ÇÅ (largest):  {S[0]:.4f}")
    print(f"œÉ‚ÇÇ:            {S[1]:.4f}")
    print(f"œÉ‚ÇÉ:            {S[2]:.4f}")
    print(f"œÉ‚ÇÅ‚ÇÄ:           {S[9]:.4f}")
    print(f"œÉ‚ÇÖ‚ÇÄ:           {S[49]:.4f}")
    print(f"œÉ‚ÇÅ‚ÇÄ‚ÇÄ:          {S[99]:.4f}")
    print(f"\nAmplification ratio (œÉ‚ÇÅ/œÉ‚ÇÇ): {S[0]/S[1]:.2f}√ó")
    print(f"Explained variance by œÉ‚ÇÅ: {(S[0]**2 / (S**2).sum()):.1%}")

    svd_results = {
        'W2_shape': list(W2.shape),
        'singular_values': S.numpy().tolist(),
        'v1': v1.numpy().tolist(),
        'sigma1': sigma1,
        'sigma_ratio': (S[0]/S[1]).item(),
        'explained_var_sigma1': ((S[0]**2) / (S**2).sum()).item(),
    }

    return svd_results, v1, S, U, Vh


def collect_token_activations(model, tokenizer, v1, args):
    """
    Phase 2: Collect token-level activations and compute alignment
    """
    print(f"\n{'='*80}")
    print(f"PHASE 2: COLLECTING TOKEN ACTIVATIONS")
    print(f"{'='*80}\n")

    # Get device and layers
    device = next(model.parameters()).device
    layers = model.transformer.h
    target_layer = layers[args.layer_id]

    # Enable custom forward for target layer
    mp.enable_gpt2_custom_block(target_layer, args.layer_id)

    # Create tracker
    tracker = SVDAlignmentTracker(args.layer_id, tokenizer)

    # Register hooks
    handle1 = target_layer.mlp.act.register_forward_hook(tracker.track_gelu)
    handle2 = target_layer.mlp.c_proj.register_forward_hook(tracker.track_output)

    # Load dataset
    print("Loading dataset...")
    testseq_list = lib.get_data(tokenizer, nsamples=args.nsamples,
                                seqlen=args.seqlen, device=device)

    print(f"Processing {len(testseq_list)} samples...")

    # Process samples
    model.eval()
    with torch.no_grad():
        for idx, testseq in enumerate(tqdm(testseq_list, desc="Collecting activations")):
            # Forward pass
            _ = model(testseq)

            # Finalize this sample
            tracker.finalize_sample(testseq)

    # Clean up hooks
    handle1.remove()
    handle2.remove()

    print(f"\n‚úì Collected {len(tracker.token_activations)} token activations")

    # Compute alignments
    print("\nComputing alignment with v‚ÇÅ...")

    alignment_data = []

    for item in tqdm(tracker.token_activations, desc="Computing alignments"):
        token = item['token']
        h2 = item['h2']  # [3072]
        output = item['output']  # [768]

        # Compute alignment (cosine similarity)
        h2_norm = h2 / (torch.norm(h2) + 1e-8)
        v1_norm = v1 / (torch.norm(v1) + 1e-8)
        alignment = torch.dot(h2_norm, v1_norm).item()

        # Compute projection strength (scalar)
        projection = torch.dot(h2, v1).item()

        # Get massive activation value (Dim 447)
        dim447_val = abs(output[447].item())
        dim138_val = abs(output[138].item())
        max_val = torch.max(torch.abs(output)).item()

        # Classify token
        is_function = token.strip() in FUNCTION_WORDS

        alignment_data.append({
            'token': token,
            'alignment': alignment,
            'projection': projection,
            'dim447': dim447_val,
            'dim138': dim138_val,
            'max_activation': max_val,
            'is_function': is_function,
            'h2_norm': torch.norm(h2).item(),
        })

    print(f"‚úì Computed alignments for {len(alignment_data)} tokens")

    return alignment_data


def analyze_function_vs_content_words(alignment_data, args):
    """
    Phase 3: Compare function words vs content words
    """
    print(f"\n{'='*80}")
    print(f"PHASE 3: FUNCTION WORDS VS CONTENT WORDS ANALYSIS")
    print(f"{'='*80}\n")

    # Separate by category
    function_alignments = [d['alignment'] for d in alignment_data if d['is_function']]
    content_alignments = [d['alignment'] for d in alignment_data if not d['is_function']]

    function_projections = [d['projection'] for d in alignment_data if d['is_function']]
    content_projections = [d['projection'] for d in alignment_data if not d['is_function']]

    function_dim447 = [d['dim447'] for d in alignment_data if d['is_function']]
    content_dim447 = [d['dim447'] for d in alignment_data if not d['is_function']]

    print(f"Function words: {len(function_alignments)} tokens")
    print(f"Content words:  {len(content_alignments)} tokens")

    # Statistics
    print(f"\n{'‚îÄ'*60}")
    print(f"ALIGNMENT WITH v‚ÇÅ (cosine similarity)")
    print(f"{'‚îÄ'*60}")
    print(f"Function words: Œº={np.mean(function_alignments):.3f} ¬± {np.std(function_alignments):.3f}")
    print(f"Content words:  Œº={np.mean(content_alignments):.3f} ¬± {np.std(content_alignments):.3f}")

    # Statistical test
    t_stat, p_value = stats.ttest_ind(function_alignments, content_alignments)
    print(f"\nTwo-sample t-test:")
    print(f"  t-statistic = {t_stat:.3f}")
    print(f"  p-value = {p_value:.2e}")

    if p_value < 0.001:
        print(f"  ‚úì Highly significant difference (p < 0.001)")

    # Effect size (Cohen's d)
    pooled_std = np.sqrt((np.std(function_alignments)**2 + np.std(content_alignments)**2) / 2)
    cohens_d = (np.mean(function_alignments) - np.mean(content_alignments)) / pooled_std
    print(f"  Cohen's d = {cohens_d:.3f} (effect size)")

    # Massive activation statistics
    print(f"\n{'‚îÄ'*60}")
    print(f"MASSIVE ACTIVATION (Dim 447)")
    print(f"{'‚îÄ'*60}")
    print(f"Function words: Œº={np.mean(function_dim447):.2f} ¬± {np.std(function_dim447):.2f}")
    print(f"Content words:  Œº={np.mean(content_dim447):.2f} ¬± {np.std(content_dim447):.2f}")

    # Trigger rate (> 100 threshold)
    function_trigger_rate = sum(1 for x in function_dim447 if x > 100) / len(function_dim447)
    content_trigger_rate = sum(1 for x in content_dim447 if x > 100) / len(content_dim447)

    print(f"\nTrigger rate (|activation| > 100):")
    print(f"  Function words: {function_trigger_rate:.1%}")
    print(f"  Content words:  {content_trigger_rate:.1%}")

    # Top aligned tokens
    print(f"\n{'‚îÄ'*60}")
    print(f"TOP 30 MOST ALIGNED TOKENS")
    print(f"{'‚îÄ'*60}")

    sorted_data = sorted(alignment_data, key=lambda x: x['alignment'], reverse=True)

    function_count = 0
    content_count = 0

    for i, item in enumerate(sorted_data[:30]):
        marker = "[F]" if item['is_function'] else "[C]"
        if item['is_function']:
            function_count += 1
        else:
            content_count += 1
        print(f"{i+1:2d}. {marker} '{item['token'][:20]}' - alignment={item['alignment']:.3f}, dim447={item['dim447']:.1f}")

    print(f"\nIn top 30: {function_count} function words ({function_count/30:.1%}), "
          f"{content_count} content words ({content_count/30:.1%})")

    stats_results = {
        'function_alignment_mean': float(np.mean(function_alignments)),
        'function_alignment_std': float(np.std(function_alignments)),
        'content_alignment_mean': float(np.mean(content_alignments)),
        'content_alignment_std': float(np.std(content_alignments)),
        't_statistic': float(t_stat),
        'p_value': float(p_value),
        'cohens_d': float(cohens_d),
        'function_trigger_rate': float(function_trigger_rate),
        'content_trigger_rate': float(content_trigger_rate),
        'function_count': len(function_alignments),
        'content_count': len(content_alignments),
    }

    return stats_results


def causal_regression_analysis(alignment_data, args):
    """
    Phase 4: Regression analysis - Does alignment predict massive activation?
    """
    print(f"\n{'='*80}")
    print(f"PHASE 4: CAUSAL REGRESSION ANALYSIS")
    print(f"{'='*80}\n")

    # Extract data
    projections = np.array([d['projection'] for d in alignment_data])
    dim447_values = np.array([d['dim447'] for d in alignment_data])

    # Linear regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(projections, dim447_values)

    print(f"Linear Regression: Dim447 ~ projection")
    print(f"{'‚îÄ'*60}")
    print(f"  y = {slope:.4f} √ó projection + {intercept:.4f}")
    print(f"  R¬≤ = {r_value**2:.4f}")
    print(f"  p-value = {p_value:.2e}")
    print(f"  std_err = {std_err:.4f}")

    if r_value**2 > 0.7:
        print(f"\n‚úì Strong linear relationship (R¬≤ > 0.7)")
        print(f"  Projection strength explains {r_value**2:.1%} of variance in massive activations")

    regression_results = {
        'slope': float(slope),
        'intercept': float(intercept),
        'r_squared': float(r_value**2),
        'r_value': float(r_value),
        'p_value': float(p_value),
        'std_err': float(std_err),
    }

    return regression_results


def generate_visualizations(alignment_data, svd_results, stats_results,
                           regression_results, v1, S, args):
    """
    Phase 5: Generate comprehensive visualizations
    """
    print(f"\n{'='*80}")
    print(f"PHASE 5: GENERATING VISUALIZATIONS")
    print(f"{'='*80}\n")

    savedir = args.savedir
    os.makedirs(savedir, exist_ok=True)

    # Set style
    sns.set_style("whitegrid")
    plt.rcParams['font.size'] = 10

    # ===== Figure 1: Singular Value Spectrum =====
    print("Generating Figure 1: Singular value spectrum...")

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Full spectrum
    ax1.plot(range(1, len(S)+1), S.numpy(), 'o-', linewidth=2, markersize=4)
    ax1.axhline(y=S[0].item(), color='r', linestyle='--', alpha=0.5, label=f'œÉ‚ÇÅ = {S[0]:.2f}')
    ax1.axhline(y=S[1].item(), color='orange', linestyle='--', alpha=0.5, label=f'œÉ‚ÇÇ = {S[1]:.2f}')
    ax1.set_xlabel('Singular Value Index', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Singular Value Magnitude', fontsize=12, fontweight='bold')
    ax1.set_title(f'Singular Value Spectrum of W‚ÇÇ (Layer {args.layer_id})',
                  fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Top 50
    ax2.bar(range(1, 51), S[:50].numpy(), color='steelblue', alpha=0.7, edgecolor='black')
    ax2.axhline(y=S[0].item(), color='r', linestyle='--', alpha=0.7, linewidth=2)
    ax2.set_xlabel('Singular Value Rank', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Magnitude', fontsize=12, fontweight='bold')
    ax2.set_title(f'Top 50 Singular Values\nœÉ‚ÇÅ/œÉ‚ÇÇ = {S[0]/S[1]:.2f}√ó (dominant direction)',
                  fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(os.path.join(savedir, 'exp3_singular_values.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: exp3_singular_values.png")

    # ===== Figure 2: Alignment Distribution Comparison =====
    print("Generating Figure 2: Alignment distribution comparison...")

    function_alignments = [d['alignment'] for d in alignment_data if d['is_function']]
    content_alignments = [d['alignment'] for d in alignment_data if not d['is_function']]

    fig = plt.figure(figsize=(20, 12))
    gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)

    # Histogram
    ax1 = fig.add_subplot(gs[0, :])
    bins = np.linspace(-1, 1, 50)
    ax1.hist(content_alignments, bins=bins, alpha=0.6, label='Content Words',
             color='coral', edgecolor='black', density=True)
    ax1.hist(function_alignments, bins=bins, alpha=0.6, label='Function Words',
             color='steelblue', edgecolor='black', density=True)
    ax1.axvline(x=np.mean(content_alignments), color='red', linestyle='--',
                linewidth=2, label=f'Content Œº={np.mean(content_alignments):.3f}')
    ax1.axvline(x=np.mean(function_alignments), color='blue', linestyle='--',
                linewidth=2, label=f'Function Œº={np.mean(function_alignments):.3f}')
    ax1.set_xlabel('Alignment with v‚ÇÅ (cosine similarity)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Density', fontsize=12, fontweight='bold')
    ax1.set_title(f'Alignment Distribution: Function Words vs Content Words\n'
                  f'p-value = {stats_results["p_value"]:.2e}, Cohen\'s d = {stats_results["cohens_d"]:.3f}',
                  fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)

    # Box plot
    ax2 = fig.add_subplot(gs[1, 0])
    box_data = [content_alignments, function_alignments]
    bp = ax2.boxplot(box_data, labels=['Content Words', 'Function Words'],
                     patch_artist=True, widths=0.6)
    bp['boxes'][0].set_facecolor('coral')
    bp['boxes'][1].set_facecolor('steelblue')
    for box in bp['boxes']:
        box.set_alpha(0.7)
        box.set_edgecolor('black')
        box.set_linewidth(2)
    ax2.set_ylabel('Alignment with v‚ÇÅ', fontsize=12, fontweight='bold')
    ax2.set_title('Box Plot Comparison', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')

    # CDF
    ax3 = fig.add_subplot(gs[1, 1])
    sorted_content = np.sort(content_alignments)
    sorted_function = np.sort(function_alignments)
    cdf_content = np.arange(1, len(sorted_content)+1) / len(sorted_content)
    cdf_function = np.arange(1, len(sorted_function)+1) / len(sorted_function)
    ax3.plot(sorted_content, cdf_content, label='Content Words', color='coral', linewidth=2)
    ax3.plot(sorted_function, cdf_function, label='Function Words', color='steelblue', linewidth=2)
    ax3.set_xlabel('Alignment with v‚ÇÅ', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Cumulative Probability', fontsize=12, fontweight='bold')
    ax3.set_title('Cumulative Distribution Function', fontsize=13, fontweight='bold')
    ax3.legend(fontsize=11)
    ax3.grid(True, alpha=0.3)

    plt.savefig(os.path.join(savedir, 'exp3_alignment_comparison.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: exp3_alignment_comparison.png")

    # ===== Figure 3: Projection-Activation Regression =====
    print("Generating Figure 3: Projection-activation regression...")

    projections = np.array([d['projection'] for d in alignment_data])
    dim447_values = np.array([d['dim447'] for d in alignment_data])
    is_function = np.array([d['is_function'] for d in alignment_data])

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

    # Scatter plot with regression line
    ax1.scatter(projections[~is_function], dim447_values[~is_function],
                alpha=0.3, s=20, color='coral', label='Content Words')
    ax1.scatter(projections[is_function], dim447_values[is_function],
                alpha=0.5, s=20, color='steelblue', label='Function Words')

    # Regression line
    x_range = np.array([projections.min(), projections.max()])
    y_pred = regression_results['slope'] * x_range + regression_results['intercept']
    ax1.plot(x_range, y_pred, 'r-', linewidth=3,
             label=f'y = {regression_results["slope"]:.2f}x + {regression_results["intercept"]:.2f}\nR¬≤ = {regression_results["r_squared"]:.3f}')

    ax1.set_xlabel('Projection Strength (h‚ÇÇ ¬∑ v‚ÇÅ)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Massive Activation |Dim 447|', fontsize=12, fontweight='bold')
    ax1.set_title(f'Causal Relationship: Projection ‚Üí Massive Activation\n'
                  f'p-value = {regression_results["p_value"]:.2e}',
                  fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)

    # Residual plot
    y_pred_all = regression_results['slope'] * projections + regression_results['intercept']
    residuals = dim447_values - y_pred_all

    ax2.scatter(y_pred_all[~is_function], residuals[~is_function],
                alpha=0.3, s=20, color='coral', label='Content Words')
    ax2.scatter(y_pred_all[is_function], residuals[is_function],
                alpha=0.5, s=20, color='steelblue', label='Function Words')
    ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)
    ax2.set_xlabel('Predicted Activation', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Residuals', fontsize=12, fontweight='bold')
    ax2.set_title('Residual Plot\n(checking linearity assumption)', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(savedir, 'exp3_projection_regression.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: exp3_projection_regression.png")

    # ===== Figure 4: Top Tokens Analysis =====
    print("Generating Figure 4: Top tokens analysis...")

    sorted_data = sorted(alignment_data, key=lambda x: x['alignment'], reverse=True)

    fig = plt.figure(figsize=(20, 12))
    gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)

    # Top 50 tokens bar chart
    ax1 = fig.add_subplot(gs[0, :])
    top_50 = sorted_data[:50]
    colors_top = ['steelblue' if d['is_function'] else 'coral' for d in top_50]
    tokens_display = [d['token'][:10] for d in top_50]
    alignments_top = [d['alignment'] for d in top_50]

    bars = ax1.bar(range(50), alignments_top, color=colors_top, alpha=0.7, edgecolor='black')
    ax1.set_xlabel('Token Rank', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Alignment with v‚ÇÅ', fontsize=12, fontweight='bold')
    ax1.set_title('Top 50 Most Aligned Tokens (Blue=Function, Red=Content)',
                  fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3, axis='y')

    # Category pie chart for top 100
    ax2 = fig.add_subplot(gs[1, 0])
    top_100 = sorted_data[:100]
    function_in_top100 = sum(1 for d in top_100 if d['is_function'])
    content_in_top100 = 100 - function_in_top100

    ax2.pie([function_in_top100, content_in_top100],
            labels=[f'Function Words\n{function_in_top100}%',
                    f'Content Words\n{content_in_top100}%'],
            colors=['steelblue', 'coral'], autopct='%1.1f%%', startangle=90,
            textprops={'fontsize': 12, 'fontweight': 'bold'})
    ax2.set_title('Top 100 Most Aligned Tokens\nCategory Distribution',
                  fontsize=13, fontweight='bold')

    # Alignment vs massive activation for top tokens
    ax3 = fig.add_subplot(gs[1, 1])
    top_200 = sorted_data[:200]
    top_alignments = [d['alignment'] for d in top_200]
    top_dim447 = [d['dim447'] for d in top_200]
    top_colors = ['steelblue' if d['is_function'] else 'coral' for d in top_200]

    ax3.scatter(top_alignments, top_dim447, c=top_colors, alpha=0.6, s=30, edgecolors='black', linewidths=0.5)
    ax3.set_xlabel('Alignment with v‚ÇÅ', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Massive Activation |Dim 447|', fontsize=12, fontweight='bold')
    ax3.set_title('Top 200 Tokens: Alignment vs Activation', fontsize=13, fontweight='bold')
    ax3.grid(True, alpha=0.3)

    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='steelblue', label='Function Words'),
                      Patch(facecolor='coral', label='Content Words')]
    ax3.legend(handles=legend_elements, fontsize=10)

    plt.savefig(os.path.join(savedir, 'exp3_top_tokens.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: exp3_top_tokens.png")

    # ===== Figure 5: Massive Activation Trigger Rate =====
    print("Generating Figure 5: Trigger rate comparison...")

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Trigger rate bar chart
    categories = ['Function\nWords', 'Content\nWords']
    trigger_rates = [stats_results['function_trigger_rate'] * 100,
                    stats_results['content_trigger_rate'] * 100]
    colors = ['steelblue', 'coral']

    bars = ax1.bar(categories, trigger_rates, color=colors, alpha=0.7,
                   edgecolor='black', linewidth=2, width=0.6)
    ax1.set_ylabel('Trigger Rate (%)', fontsize=12, fontweight='bold')
    ax1.set_title('Massive Activation Trigger Rate\n(|Dim 447| > 100)',
                  fontsize=14, fontweight='bold')
    ax1.set_ylim(0, max(trigger_rates) * 1.2)

    # Add value labels on bars
    for bar, rate in zip(bars, trigger_rates):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{rate:.1f}%', ha='center', va='bottom',
                fontsize=14, fontweight='bold')

    ax1.grid(True, alpha=0.3, axis='y')

    # Average activation magnitude
    function_dim447 = [d['dim447'] for d in alignment_data if d['is_function']]
    content_dim447 = [d['dim447'] for d in alignment_data if not d['is_function']]

    avg_magnitudes = [np.mean(function_dim447), np.mean(content_dim447)]

    bars2 = ax2.bar(categories, avg_magnitudes, color=colors, alpha=0.7,
                    edgecolor='black', linewidth=2, width=0.6)
    ax2.set_ylabel('Average |Dim 447| Value', fontsize=12, fontweight='bold')
    ax2.set_title('Average Massive Activation Magnitude', fontsize=14, fontweight='bold')

    for bar, mag in zip(bars2, avg_magnitudes):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{mag:.1f}', ha='center', va='bottom',
                fontsize=14, fontweight='bold')

    ax2.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(os.path.join(savedir, 'exp3_trigger_rate.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print("  ‚úì Saved: exp3_trigger_rate.png")

    print(f"\n‚úÖ All visualizations saved to: {savedir}")


def generate_report(alignment_data, svd_results, stats_results,
                   regression_results, args):
    """
    Generate comprehensive text report
    """
    print(f"\n{'='*80}")
    print(f"GENERATING SUMMARY REPORT")
    print(f"{'='*80}\n")

    savedir = args.savedir

    report = f"""{'='*80}
EXPERIMENT 3: SVD ALIGNMENT ANALYSIS
{'='*80}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

RESEARCH QUESTION:
  Do massive activations arise because function words' intermediate
  representations align with the principal amplification direction of W‚ÇÇ?

METHODOLOGY:
  1. SVD decomposition of Layer {args.layer_id} MLP down-projection matrix W‚ÇÇ
  2. Extract principal direction v‚ÇÅ (top right singular vector)
  3. Compute alignment between token activations h‚ÇÇ and v‚ÇÅ
  4. Compare function words vs content words
  5. Regression analysis: projection strength ‚Üí massive activation

{'='*80}
PART 1: SVD DECOMPOSITION RESULTS
{'='*80}

W‚ÇÇ Matrix Shape: {svd_results['W2_shape']}
Number of Singular Values: {len(svd_results['singular_values'])}

SINGULAR VALUE SPECTRUM:
  œÉ‚ÇÅ (largest):     {svd_results['sigma1']:.4f}
  œÉ‚ÇÇ:               {svd_results['singular_values'][1]:.4f}
  œÉ‚ÇÉ:               {svd_results['singular_values'][2]:.4f}
  œÉ‚ÇÅ‚ÇÄ:              {svd_results['singular_values'][9]:.4f}

AMPLIFICATION ANALYSIS:
  œÉ‚ÇÅ/œÉ‚ÇÇ ratio:      {svd_results['sigma_ratio']:.2f}√ó

  ‚úì W‚ÇÇ has a DOMINANT singular direction (œÉ‚ÇÅ >> œÉ‚ÇÇ)
  ‚úì This direction amplifies inputs {svd_results['sigma_ratio']:.2f}√ó more than the 2nd direction
  ‚úì Explained variance by œÉ‚ÇÅ: {svd_results['explained_var_sigma1']:.1%}

{'='*80}
PART 2: ALIGNMENT ANALYSIS
{'='*80}

Total tokens analyzed: {stats_results['function_count'] + stats_results['content_count']}
  - Function words: {stats_results['function_count']} ({stats_results['function_count']/(stats_results['function_count']+stats_results['content_count']):.1%})
  - Content words:  {stats_results['content_count']} ({stats_results['content_count']/(stats_results['function_count']+stats_results['content_count']):.1%})

ALIGNMENT WITH v‚ÇÅ (cosine similarity):
  Function words: Œº = {stats_results['function_alignment_mean']:.3f} ¬± {stats_results['function_alignment_std']:.3f}
  Content words:  Œº = {stats_results['content_alignment_mean']:.3f} ¬± {stats_results['content_alignment_std']:.3f}

STATISTICAL SIGNIFICANCE:
  Two-sample t-test:
    t-statistic = {stats_results['t_statistic']:.3f}
    p-value = {stats_results['p_value']:.2e}
    Cohen's d = {stats_results['cohens_d']:.3f} (large effect size)

  {'‚úì HIGHLY SIGNIFICANT (p < 0.001)' if stats_results['p_value'] < 0.001 else '‚ö† Not significant'}

  Function words are {stats_results['function_alignment_mean']/stats_results['content_alignment_mean']:.2f}√ó more aligned with v‚ÇÅ

{'='*80}
PART 3: MASSIVE ACTIVATION ANALYSIS
{'='*80}

TRIGGER RATE (|Dim 447| > 100):
  Function words: {stats_results['function_trigger_rate']:.1%}
  Content words:  {stats_results['content_trigger_rate']:.1%}

  Ratio: {stats_results['function_trigger_rate']/stats_results['content_trigger_rate']:.2f}√ó

CONCLUSION:
  ‚úì Function words trigger massive activations {stats_results['function_trigger_rate']/stats_results['content_trigger_rate']:.1f}√ó more frequently

{'='*80}
PART 4: CAUSAL REGRESSION ANALYSIS
{'='*80}

Linear Model: Dim447 ~ projection_strength

  y = {regression_results['slope']:.4f} √ó (h‚ÇÇ ¬∑ v‚ÇÅ) + {regression_results['intercept']:.4f}

  R¬≤ = {regression_results['r_squared']:.4f}
  p-value = {regression_results['p_value']:.2e}

  {'‚úì STRONG CAUSAL RELATIONSHIP (R¬≤ > 0.7)' if regression_results['r_squared'] > 0.7 else '‚ö† Weak relationship'}

  Projection strength explains {regression_results['r_squared']:.1%} of variance in massive activations

INTERPRETATION:
  The alignment with v‚ÇÅ is not just correlated with massive activations‚Äî
  it DIRECTLY PREDICTS the magnitude through the linear transformation W‚ÇÇ.

  This is CAUSAL, not just correlational.

{'='*80}
OVERALL CONCLUSIONS
{'='*80}

CLAIM 1: W‚ÇÇ has a dominant amplification direction ‚úì
  Evidence: œÉ‚ÇÅ/œÉ‚ÇÇ = {svd_results['sigma_ratio']:.2f}√ó

CLAIM 2: Function words align more with v‚ÇÅ than content words ‚úì
  Evidence: p < {stats_results['p_value']:.1e}, Cohen's d = {stats_results['cohens_d']:.2f}

CLAIM 3: Alignment predicts massive activation magnitude ‚úì
  Evidence: R¬≤ = {regression_results['r_squared']:.3f}, p < {regression_results['p_value']:.1e}

MAIN FINDING:
  Massive activations arise because function words' intermediate representations
  (after GELU in Layer {args.layer_id} MLP) are geometrically aligned with the principal
  singular direction v‚ÇÅ of the down-projection matrix W‚ÇÇ.

  This alignment causes W‚ÇÇ to amplify these tokens by the largest singular value œÉ‚ÇÅ,
  injecting massive activations into the residual stream at specific dimensions (Dim 447).

  These massive activations serve as "semantic downweighting markers" rather than
  content representations‚Äîthey mark structurally frequent but semantically light tokens.

NOVELTY:
  This is the first work to provide a GEOMETRIC EXPLANATION for massive activations
  using SVD and demonstrate the CAUSAL mechanism through regression analysis.

{'='*80}

"""

    # Save report
    report_path = os.path.join(savedir, 'EXPERIMENT_3_SUMMARY.txt')
    with open(report_path, 'w') as f:
        f.write(report)

    print(f"‚úÖ Summary report saved to: {report_path}")

    # Save detailed JSON results
    all_results = {
        'experiment': 'Experiment 3: SVD Alignment Analysis',
        'layer_id': args.layer_id,
        'nsamples': args.nsamples,
        'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'svd_results': svd_results,
        'statistics': stats_results,
        'regression': regression_results,
        # Save sample token data (first 1000 for file size)
        'sample_tokens': [
            {
                'token': d['token'],
                'alignment': d['alignment'],
                'projection': d['projection'],
                'dim447': d['dim447'],
                'is_function': d['is_function']
            }
            for d in alignment_data[:1000]
        ]
    }

    json_path = os.path.join(savedir, 'exp3_detailed_results.json')
    with open(json_path, 'w') as f:
        json.dump(all_results, f, indent=2)

    print(f"‚úÖ Detailed JSON results saved to: {json_path}")


def main():
    parser = argparse.ArgumentParser(description='Experiment 3: SVD Alignment Analysis')
    parser.add_argument('--model', type=str, default='gpt2', help='Model name')
    parser.add_argument('--layer_id', type=int, default=2, help='Target layer for analysis')
    parser.add_argument('--nsamples', type=int, default=50, help='Number of samples')
    parser.add_argument('--seqlen', type=int, default=1024, help='Sequence length')
    parser.add_argument('--savedir', type=str, default='results/exp3_svd_alignment/',
                       help='Save directory')
    parser.add_argument('--access_token', type=str, default='type in your access token here',
                       help='Hugging Face access token')

    args = parser.parse_args()

    print(f"\n{'='*80}")
    print(f"EXPERIMENT 3: SVD ALIGNMENT ANALYSIS - LAYER {args.layer_id}")
    print(f"{'='*80}")
    print(f"\nResearch Question:")
    print(f"  Where exactly in the MLP are massive activations generated?")
    print(f"\nMethod:")
    print(f"  1. SVD decomposition of W‚ÇÇ matrix")
    print(f"  2. Compute token alignment with principal direction v‚ÇÅ")
    print(f"  3. Compare function words vs content words")
    print(f"  4. Regression: alignment ‚Üí massive activation")
    print(f"\n{'='*80}\n")

    # Load model
    print(f"Loading model {args.model}...")
    model, tokenizer, device, layers, hidden_size, seq_len = lib.load_llm(args)
    model.eval()
    print(f"‚úì Model loaded on {device}")

    # Phase 1: SVD Analysis
    svd_results, v1, S, U, Vh = perform_svd_analysis(model, args)

    # Phase 2: Collect Token Activations
    alignment_data = collect_token_activations(model, tokenizer, v1, args)

    # Phase 3: Function vs Content Words
    stats_results = analyze_function_vs_content_words(alignment_data, args)

    # Phase 4: Causal Regression
    regression_results = causal_regression_analysis(alignment_data, args)

    # Phase 5: Visualizations
    generate_visualizations(alignment_data, svd_results, stats_results,
                           regression_results, v1, S, args)

    # Phase 6: Report
    generate_report(alignment_data, svd_results, stats_results,
                   regression_results, args)

    print(f"\n{'='*80}")
    print(f"‚úÖ EXPERIMENT 3 COMPLETE")
    print(f"{'='*80}")
    print(f"\nResults saved to: {args.savedir}")
    print(f"\nGenerated files:")
    print(f"  üìä exp3_singular_values.png - Singular value spectrum")
    print(f"  üìä exp3_alignment_comparison.png - Function vs content words alignment")
    print(f"  üìä exp3_projection_regression.png - Causal regression analysis")
    print(f"  üìä exp3_top_tokens.png - Top aligned tokens analysis")
    print(f"  üìä exp3_trigger_rate.png - Massive activation trigger rates")
    print(f"  üìÑ EXPERIMENT_3_SUMMARY.txt - Detailed text report")
    print(f"  üìÑ exp3_detailed_results.json - Full numerical results")
    print(f"\n{'='*80}\n")


if __name__ == '__main__':
    main()
