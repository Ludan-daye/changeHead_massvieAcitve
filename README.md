# Attention Head Pruning and Massive Activations Analysis

<div align="center">

![GPT-2](https://img.shields.io/badge/Model-GPT--2-blue)
![Python](https://img.shields.io/badge/Python-3.12-green)
![PyTorch](https://img.shields.io/badge/PyTorch-2.9.0-red)
![License](https://img.shields.io/badge/License-MIT-yellow)

**Investigating the relationship between attention heads and massive activations in large language models**

[ğŸ“– Overview](#overview) â€¢ [ğŸ”¬ Key Findings](#key-findings) â€¢ [ğŸ“Š Visualizations](#visualizations) â€¢ [ğŸš€ Quick Start](#quick-start) â€¢ [ä¸­æ–‡ç‰ˆ](README_CN.md)

</div>

---

## ğŸ“– Overview

This research explores the phenomenon of **"Massive Activations"** in GPT-2 and investigates whether attention heads are responsible for generating these extreme activation values. Through systematic experiments, we identify which heads focus on massive activations and test whether pruning them affects these activations.

### Research Questions

1. **What are massive activations?** Certain feature dimensions exhibit activation values 300-3000Ã— larger than the median.
2. **Which attention heads focus on these activations?** We analyze all 144 heads (12 layers Ã— 12 heads) in GPT-2.
3. **Do these heads generate massive activations?** We prune the most important heads and measure the impact.

### ğŸ¯ Surprising Discovery

**Massive activations are NOT generated by attention heads!** Even pruning the highest-scoring heads (attention score: 0.828) results in **0% change** in massive activation magnitudes. This suggests massive activations are an emergent network property, likely originating from MLP layers or multi-layer interactions.

---

## ğŸ”¬ Key Findings

### 1ï¸âƒ£ Massive Activations Exist and Follow Clear Patterns

<div align="center">

![Layer Analysis](results/llm/layerwise/gpt2.png)

**Figure 1: Massive Activation Magnitude Across Layers**

</div>

#### Observations:
- **Early Layers (0-1)**: Low activation values (<1000)
- **Middle Layers (2-4)**: Rapid growth phase (~2500)
- **Deep Layers (5-10)**: Stable high plateau (~3000)
- **Final Layer (11)**: Sharp drop (~400)
- **Top1/Median Ratio**: 300-3000Ã— across layers

#### Statistics:

| Layer | Top 1 | Top 2 | Top 3 | Median | Ratio |
|-------|-------|-------|-------|--------|-------|
| 0     | 101   | 100   | 50    | 0.60   | 168Ã—  |
| 2     | 2475  | 703   | 101   | 0.84   | 2946Ã— |
| 5     | 2890  | 834   | 179   | 1.28   | 2258Ã— |
| 10    | 3019  | 862   | 214   | 2.61   | 1157Ã— |

---

### 2ï¸âƒ£ Attention Heads Show Clear Specialization

<div align="center">

![Head Analysis](results/head_analysis/gpt2_head_analysis.png)

**Figure 2: Attention Head Focus on Massive Activation Tokens (First Token)**

</div>

We measured how much each head attends to the first token (where massive activations commonly appear).

#### Top 10 Most Important Heads:

| Rank | Layer | Head | Attention Score | Description |
|------|-------|------|-----------------|-------------|
| ğŸ¥‡ 1 | 5     | 1    | **0.828**       | Highest overall |
| ğŸ¥ˆ 2 | 6     | 1    | 0.796           | Deep layer specialist |
| ğŸ¥‰ 3 | 7     | 2    | 0.796           | Deep layer specialist |
| 4    | 10    | 5    | 0.737           | Late layer focus |
| 5    | 5     | 6    | 0.726           | Mid-layer cluster |
| 6    | 7     | 4    | 0.733           | Deep layer |
| 7    | 5     | 8    | 0.720           | Mid-layer cluster |
| 8    | 6     | 9    | 0.701           | Deep layer |
| 9    | 2     | 7    | 0.568           | Early important head |
| 10   | 8     | 4    | 0.675           | Deep layer |

#### Bottom 5 Least Important Heads (Safe to Prune):

| Rank | Layer | Head | Attention Score | Status |
|------|-------|------|-----------------|--------|
| 1    | 0     | 1    | 0.001           | âœ… Minimal impact |
| 2    | 4     | 11   | 0.002           | âœ… Minimal impact |
| 3    | 11    | 8    | 0.001           | âœ… Minimal impact |
| 4    | 1     | 10   | 0.002           | âœ… Minimal impact |
| 5    | 0     | 3    | 0.002           | âœ… Minimal impact |

<div align="center">

![Head Ranking](results/head_analysis/gpt2_head_ranking.png)

**Figure 3: Head Importance Patterns Across Layers**

</div>

---

### 3ï¸âƒ£ Pruning Top Heads Has Minimal Impact on Massive Activations

<div align="center">

![Pruning Summary](results/head_pruning_massive/summary_comparison.png)

**Figure 4: Impact Comparison - Pruning Different Heads**

</div>

#### Experiment Design:

**Experiment A: Prune TOP Heads** (Most related to massive activations)
- Layer 2, Head 7 (score: 0.568)
- Layer 5, Head 1 (score: 0.828) â­ Highest scoring head
- Layer 6, Head 1 (score: 0.796)

**Experiment B: Prune BOTTOM Heads** (Least related)
- Layer 0, Head 1 (score: 0.001)
- Layer 4, Head 11 (score: 0.002)
- Layer 11, Head 8 (score: 0.001)

#### Results:

| Experiment | Avg Change | Max Change | Impact |
|-----------|-----------|-----------|---------|
| **Prune TOP Heads** | **+0.06%** | +0.73% (Layer 11) | âš ï¸ Almost none |
| **Prune BOTTOM Heads** | **-0.57%** | -2.64% (Layer 0) | â— 7Ã— larger! |

**Unexpected Finding**: Pruning "unimportant" bottom heads has **7Ã— more impact** than pruning top heads!

<details>
<summary>ğŸ“Š Click to see detailed comparison plots</summary>

<div align="center">

![Top Heads Pruning](results/head_pruning_massive/Prune_TOP_Heads_comparison.png)

**Figure 5: Detailed Impact of Pruning TOP Heads**

![Bottom Heads Pruning](results/head_pruning_massive/Prune_BOTTOM_Heads_comparison.png)

**Figure 6: Detailed Impact of Pruning BOTTOM Heads**

</div>

</details>

#### Key Observations:

1. **Massive activations are highly robust** to pruning individual heads
2. **Top heads are "readers" not "generators"** - they focus on existing massive activations but don't create them
3. **Early layer heads have systemic impact** - even "unimportant" early heads affect all downstream layers
4. **Network has compensation mechanisms** - when important heads are removed, others compensate

---

### 4ï¸âƒ£ 3D Visualization: Before vs After Pruning

<div align="center">

![Layer 2 Comparison](results/3d_comparison/layer2_3d_comparison.png)

**Figure 7: Layer 2 - Before (left) vs After Pruning Head 7 (right)**

</div>

#### Layer 2 Analysis:
- **Pruned**: Head 7 (highest attention score for this layer: 0.568)
- **Change in max activation**: **-0.08%** (2480 â†’ 2478)
- **Visual difference**: Almost identical bar heights and positions

<div align="center">

![Layer 2 Difference](results/3d_comparison/layer2_difference_analysis.png)

**Figure 8: Layer 2 Difference Analysis**

</div>

---

<div align="center">

![Layer 5 Comparison](results/3d_comparison/layer5_3d_comparison.png)

**Figure 9: Layer 5 - Before (left) vs After Pruning Head 1 (right)**

</div>

#### Layer 5 Analysis:
- **Pruned**: Head 1 (highest attention score in entire model: 0.828)
- **Change in max activation**: **0.00%** (2898 â†’ 2898)
- **Visual difference**: Completely identical! Even pruning the most important head changes nothing.

<div align="center">

![Layer 5 Difference](results/3d_comparison/layer5_difference_analysis.png)

**Figure 10: Layer 5 Difference Analysis - Zero Impact**

</div>

#### 3D Visualization Insights:

1. **Spatial patterns unchanged**: Same tokens, same feature dimensions, same magnitudes
2. **Feature dimension 138, 378**: Massive activations remain at ~1000 even after pruning
3. **No compensation artifacts**: No new peaks or shifts in other dimensions
4. **Rock-solid stability**: Massive activations are intrinsic to the network architecture

---

## ğŸ’¡ Conclusions

### Main Findings

âœ… **Massive activations are real and follow predictable layer-wise patterns**
- Emerge in layer 2, peak in layers 5-10, drop in layer 11
- Top activations are 300-3000Ã— larger than median values

âœ… **Attention heads specialize in different roles**
- Some heads heavily focus on massive activation tokens
- Others largely ignore them
- Clear differentiation across all 144 heads

âœ… **Massive activations are NOT generated by attention heads**
- Pruning the highest-scoring heads (0.828 attention score) â†’ 0% change
- Even removing multiple top heads has minimal impact
- Suggests MLP layers or cross-layer interactions as the source

âœ… **Network exhibits robustness and compensation**
- Removing important heads triggers compensation mechanisms
- Early layer stability is crucial for downstream processing
- System-level property, not component-level

### Implications

ğŸ”¬ **For Model Understanding**:
- Massive activations are an emergent property of the entire network
- Attention mechanisms utilize but don't create these activations
- MLP layers warrant deeper investigation

ğŸ› ï¸ **For Model Compression**:
- Some heads can be safely pruned without affecting massive activations
- Early layer heads are more critical than expected
- Head importance â‰  pruning impact

ğŸ¯ **For Future Research**:
- Investigate MLP layer contributions to massive activations
- Study multi-head compensation mechanisms
- Test cumulative effects of pruning many heads
- Evaluate impact on downstream task performance (perplexity, accuracy)

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.12+
# CUDA-capable GPU (recommended)

# Create virtual environment
python3 -m venv massive-activations-env
source massive-activations-env/bin/activate  # On Windows: massive-activations-env\Scripts\activate

# Install dependencies
pip install torch torchvision transformers timm accelerate datasets matplotlib seaborn sentencepiece protobuf
```

### Running the Experiments

#### 1ï¸âƒ£ Basic Massive Activation Analysis (Exp1 & Exp2)

```bash
# Experiment 1: 3D feature visualization for a specific layer
python main_llm.py --model gpt2 --exp1 --layer_id 2 --savedir results/llm/3d_feat_vis/

# Experiment 2: Layer-wise analysis across all layers
python main_llm.py --model gpt2 --exp2 --savedir results/llm/layerwise/
```

#### 2ï¸âƒ£ Attention Head Analysis

```bash
# Analyze which heads focus on massive activations
python analyze_heads_simple.py --model gpt2 --nsamples 30 --savedir results/head_analysis/
```

**Output**:
- `gpt2_head_analysis.png`: Heatmap showing attention scores for each head
- `gpt2_head_ranking.png`: Ranking visualization across layers
- `gpt2_pruning_config.txt`: Configuration file for pruning experiments

#### 3ï¸âƒ£ Head Pruning Impact on Massive Activations

```bash
# Test how pruning different heads affects massive activations
python test_head_pruning_on_massive.py --model gpt2 --nsamples 20 --savedir results/head_pruning_massive/
```

**This runs two experiments**:
- Prune TOP heads (most related to massive activations)
- Prune BOTTOM heads (least related)

**Output**:
- Comparison plots showing before/after massive activation magnitudes
- Summary statistics and analysis

#### 4ï¸âƒ£ 3D Comparison: Before vs After Pruning

```bash
# Layer 2: Compare before and after pruning Head 7
python compare_3d_before_after_pruning.py --model gpt2 --layer_id 2 --savedir results/3d_comparison/

# Layer 5: Compare before and after pruning Head 1 (highest scoring head)
python compare_3d_before_after_pruning.py --model gpt2 --layer_id 5 --savedir results/3d_comparison/
```

**Output**:
- Side-by-side 3D visualizations
- Difference heatmaps and analysis

---

## ğŸ“š Related Work

This work is based on the paper:

**"Massive Activations in Large Language Models"**
- Paper: https://arxiv.org/abs/2402.17762
- Original Code: https://github.com/locuslab/massive-activations

### Key Extensions in This Work:

1. âœ… Added GPT-2 support (original code focused on LLaMA)
2. âœ… Implemented attention head analysis and ranking
3. âœ… Created head pruning framework with impact measurement
4. âœ… Generated 3D before/after comparison visualizations
5. âœ… Discovered that attention heads are "readers" not "generators" of massive activations

---

## ğŸ“„ License

MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Original "Massive Activations" paper and codebase
- Hugging Face Transformers library
- OpenAI for GPT-2

---

<div align="center">

**ğŸŒŸ If you find this work interesting, please star the repository! ğŸŒŸ**

Made with â¤ï¸ for understanding massive activations in LLMs

</div>
